import openai
import requests
from bs4 import BeautifulSoup
import socket
import requests.exceptions
import urllib.parse
from collections import deque
from .openai_request import generate_response
# from IPython.display import display, Markdown

# website scanner driver
def website_scanner(ulr_id, scan_id, current_page = True):
    basic_url = ulr_id.split('/')[:3]
    user_url = ulr_id.strip()
    urls = deque([user_url])
    scraped_urls = set()
    output = {}
    unchecked_urls = {}
    scanned_ports = ''
    count = 0
    response = None

    try:
        print("[*] Performing Scan..!")
        while len(urls):
            count += 1
            url = urls.popleft()
            scraped_urls.add(url)

            parts = urllib.parse.urlsplit(url)
            base_url = '{0.scheme}://{0.netloc}'.format(parts)

            path = url[:url.rfind('/')+1] if '/' in parts.path else url

            print('[%d] Processing %s' % (count, url))
            try:
                response = requests.get(url)
                output[url] = generate_response(url, scan_id)
                print()
                print("[->] Response for the vulnerability is as follows ")
                # for line in output[url]:
                #     print(line.choices[0].delta["content"], end="", flush=True)
                print(output[url].choices[0].message["content"])
            except Exception as e:
                print(e)
                unchecked_urls[url] = e

            if response is not None:
                soup = BeautifulSoup(response.text, features="lxml")

                for anchor in soup.find_all("a"):
                    link = anchor.attrs['href'] if 'href' in anchor.attrs else ''
                    if link.startswith('/'):
                        link = base_url + link
                    elif not link.startswith('http'):
                        link = path + link
                    if not link in urls and not link in scraped_urls:
                        if basic_url[2] in link:
                            urls.append(link)
                        else:
                            unchecked_urls[link] = "Unrelated url"
                            
            if current_page == True or count == 20:
                break
    except KeyboardInterrupt:
        pass

