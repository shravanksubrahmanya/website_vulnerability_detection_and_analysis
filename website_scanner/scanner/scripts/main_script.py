import openai
import requests
from bs4 import BeautifulSoup
import socket
import requests.exceptions
import urllib.parse
from collections import deque
from .openai_request import generate_response
# from IPython.display import display, Markdown

from scanner.models import ProcessDetail, UserScanMap, ScanDetail

# website scanner driver
def website_scanner(ulr_id, scan_id, current_page = 1):
    basic_url = ulr_id.split('/')[:3]
    user_url = ulr_id.strip()
    urls = deque([user_url])
    scraped_urls = set()
    output = {}
    unchecked_urls = {}
    scanned_ports = ''
    count = 0
    response = None
    total_requests_count = 0
    user_scan_map = UserScanMap.objects.filter(id = scan_id).first()
    scan_type = "Full Scan"

    try:
        message = "[*] Performing Scan..!"
        print(message)
        while len(urls):
            count += 1
            url = urls.popleft()
            scraped_urls.add(url)

            parts = urllib.parse.urlsplit(url)
            base_url = '{0.scheme}://{0.netloc}'.format(parts)

            path = url[:url.rfind('/')+1] if '/' in parts.path else url

            message = '[%d] Processing %s' % (count, url)
            print(message)
            process_detail = ProcessDetail(scan_id = user_scan_map, process_desc = message)
            process_detail.save()
            try:
                response = requests.get(url)
                total_requests_count += 1
                output[url], requests_count = generate_response(url, scan_id)
                print(requests_count, "requests count")
                total_requests_count += requests_count
                print()
                print("[->] Response for the vulnerability is as follows ")
                # for line in output[url]:
                #     print(line.choices[0].delta["content"], end="", flush=True)
                print(output[url].choices[0].message["content"])
            except Exception as e:
                print(e)
                # scan_detail = ScanDetail(scan_id=user_scan_map, url=url, status=1, scan_type = scan_type)
                # scan_detail.save()
                unchecked_urls[url] = e

            if response is not None:
                soup = BeautifulSoup(response.text, features="lxml")

                for anchor in soup.find_all("a"):
                    link = anchor.attrs['href'] if 'href' in anchor.attrs else ''
                    if link.startswith('/'):
                        link = base_url + link
                    elif not link.startswith('http'):
                        link = path + link
                    if not link in urls and not link in scraped_urls:
                        if basic_url[2] in link:
                            urls.append(link)
                        else:
                            unchecked_urls[link] = "Unrelated url"
            
            message = 'Processing complete'
            print(message)
            process_detail = ProcessDetail(scan_id = user_scan_map, process_desc = message)
            process_detail.save()
            
            if(current_page != 2) or count >= 6:
                break

    except KeyboardInterrupt:
        pass
    
    return total_requests_count
